

# Gerador de Base Fake para Big Data / ETL

## ğŸ“Œ VisÃ£o Geral

Este projeto fornece um **gerador de dados sintÃ©ticos em larga escala**, ideal para testes de:

* Pipelines ETL/ELT
* Big Data (Spark, Hadoop, Hive, Presto, DuckDB, etc.)
* Data Warehouses (Redshift, BigQuery, Snowflake)
* Modelagem dimensional (DW)
* AnÃ¡lises de performance e ingestÃ£o massiva

O script cria **dimensÃµes realistas** e uma tabela fato com **dezenas ou centenas de milhÃµes de linhas**, gravadas em arquivos **Parquet particionados** e gerados em *chunks* para evitar estouro de memÃ³ria.

---

## ğŸš€ Funcionalidades

âœ” Gera dimensÃµes:

* `dim_clientes` (1 milhÃ£o de registros)
* `dim_produtos` (50 mil registros)
* `dim_lojas` (5 mil registros)

âœ” Cria uma tabela:

* `fato_vendas` com atÃ© **bilhÃµes de linhas**, geradas por chunks

âœ” ProduÃ§Ã£o em formato **Parquet**
âœ” Dados simulados com:

* DistribuiÃ§Ãµes mais realistas (quantidade, valores, datas)
* Relacionamentos consistentes entre dimensÃµes e fato
* Uso do Faker para gerar informaÃ§Ãµes brasileiras

âœ” OrganizaÃ§Ã£o em diretÃ³rios e particionamento por:

* Ano
* MÃªs

âœ” Projeto ideal para testes de:

* Performance de leitura
* Carga de dados massiva
* Particionamento
* Processamento distribuÃ­do

---

## ğŸ—‚ Estrutura dos Dados Gerados

```
data_fake_bigdata/
â”‚
â”œâ”€â”€ dim_clientes.parquet
â”œâ”€â”€ dim_produtos.parquet
â”œâ”€â”€ dim_lojas.parquet
â”‚
â””â”€â”€ fato_vendas/
    â”œâ”€â”€ fato_vendas_chunk_0000.parquet
    â”œâ”€â”€ fato_vendas_chunk_0001.parquet
    â””â”€â”€ ...
```

Cada chunk possui, por padrÃ£o, **1 milhÃ£o de linhas**, podendo ser ajustado.

---

## ğŸ“¦ InstalaÃ§Ã£o das DependÃªncias

```bash
pip install faker pyarrow pandas
```

---

## ğŸ§  Como Funciona o Gerador

O script segue trÃªs etapas principais:

### **1) Criar dimensÃµes**

Registra clientes, produtos e lojas com dados sintÃ©ticos.

### **2) Carregar apenas os IDs em memÃ³ria**

Isso evita uso excessivo de RAM.

### **3) Gerar a tabela fato por chunks**

Cada chunk contÃ©m:

* FK de cliente, produto e loja
* Data da venda
* Quantidade (com distribuiÃ§Ã£o exponencial)
* Valor unitÃ¡rio puxado da dim_produtos
* Valor total calculado

Cada chunk Ã© salvo como Parquet, permitindo escalar para centenas de milhÃµes de linhas.

---

## ğŸ§ª Exemplos de Volumes PossÃ­veis

| Tabela       | Linhas sugeridas | ObservaÃ§Ã£o             |
| ------------ | ---------------- | ---------------------- |
| dim_clientes | 1.000.000        | Pode aumentar para 10M |
| dim_produtos | 50.000           |                        |
| dim_lojas    | 5.000            |                        |
| fato_vendas  | 50.000.000+      | AjustÃ¡vel atÃ© bilhÃµes  |

Quantidade **nÃ£o depende de RAM**, apenas de disco e tempo de execuÃ§Ã£o.

---

## ğŸ— Como Executar

### ExecuÃ§Ã£o padrÃ£o:

```bash
python gerar_base_fake.py
```

O script irÃ¡:

1. Criar as dimensÃµes
2. Gerar 50 milhÃµes de linhas para a tabela fato
3. Gravar tudo dentro da pasta `data_fake_bigdata/`

---

## âš™ ConfiguraÃ§Ãµes Importantes

No final do script:

```python
gerar_dim_clientes(n_clientes=1_000_000)
gerar_dim_produtos(n_produtos=50_000)
gerar_dim_lojas(n_lojas=5_000)

gerar_fato_vendas(
    total_linhas=50_000_000,
    chunk_size=1_000_000,
)
```

### Alterar volume total:

```python
total_linhas=500_000_000  # 500 milhÃµes
```

### Alterar tamanho dos chunks:

```python
chunk_size=5_000_000
```

### Pequenos testes:

```python
total_linhas=1_000_000
chunk_size=200_000
```

---

## ğŸ“Š Exemplos de Uso em Ferramentas de Big Data

### **Spark**

```python
df = spark.read.parquet("data_fake_bigdata/fato_vendas")
df.groupBy("ano", "mes").sum("valor_total").show()
```

### **DuckDB**

```sql
SELECT ano, mes, SUM(valor_total)
FROM 'data_fake_bigdata/fato_vendas/*.parquet'
GROUP BY 1, 2;
```


## ğŸ“œ LicenÃ§a

MIT License â€“ Utilize e modifique livremente.